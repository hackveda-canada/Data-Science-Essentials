{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Science Essentials Day 9: Stemming.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hackveda-canada/Data-Science-Essentials/blob/master/Data_Science_Essentials_Day_9_Stemming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "h_d_f1tF2a29",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PxMGrxF32lU5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**STEMMING**\n",
        "-----------------\n",
        "\n",
        "a) Porter Algorithm\n",
        "b) Lancaster Algorithm\n",
        "c) Snowball Algorithm\n",
        "\n",
        "\n",
        "Process of removing commonly used words in english which does not qualify as word, and display the root word.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "xDpGzyvY4CmB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "ed3a8c6f-92eb-4d89-a508-b7fbaa131d78"
      },
      "cell_type": "code",
      "source": [
        "# Process: Stemming\n",
        "# Transform the original text into root text by removal of affixes\n",
        "\n",
        "# Porter Stemmer Algorithm\n",
        "# Lancaster Stemmer Algorithm\n",
        "# Snowball Stemmer Algorithm\n",
        "\n",
        "# Lets consider a text from a source\n",
        "text = \"caring cares cared carefully careful carelessly fishing fishes\"\n",
        "\n",
        "# Objective to remove affixes and clean up the words\n",
        "# Strip affixes from the token and return the stem\n",
        "\n",
        "#Import libraries\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
        "\n",
        "\n",
        "#Step 1: Get word tokens from statements\n",
        "from nltk import word_tokenize\n",
        "word_tokenize(text)\n",
        "\n",
        "# Step 2: Perform stemming on a word_token\n",
        "porter_stemmer = PorterStemmer()\n",
        "porter_stemmer.stem(\"cared\")\n",
        "\n",
        "\n",
        "snowball_stemmer = SnowballStemmer(language=\"english\")\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "\n",
        "#Step 2a: Perform stemming on word_tokens\n",
        "word_tokens = word_tokenize(text)\n",
        "print(word_tokens)\n",
        "\n",
        "\n",
        "porter_root_words = []\n",
        "snowball_root_words = []\n",
        "lancaster_root_words = []\n",
        "\n",
        "for token in word_tokens:\n",
        "  porter_root_word = porter_stemmer.stem(token)\n",
        "  porter_root_words.append(porter_root_word)\n",
        "  snowball_root_word = snowball_stemmer.stem(token)\n",
        "  snowball_root_words.append(snowball_root_word)\n",
        "  lancaster_root_word = lancaster_stemmer.stem(token)\n",
        "  lancaster_root_words.append(lancaster_root_word)\n",
        "\n",
        "print(\"Original Words - \",text)\n",
        "print(\"Portar Root Words - \",\" \".join(porter_root_words))\n",
        "print(\"Snowball Root Words - \",\" \".join(snowball_root_words))\n",
        "print(\"Lancaster Root Words - \",\" \".join(lancaster_root_words))\n",
        "  "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['caring', 'cares', 'cared', 'carefully', 'careful', 'carelessly', 'fishing', 'fishes']\n",
            "Original Words -  caring cares cared carefully careful carelessly fishing fishes\n",
            "Portar Root Words -  care care care care care carelessli fish fish\n",
            "Snowball Root Words -  care care care care care careless fish fish\n",
            "Lancaster Root Words -  car car car car car careless fish fish\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}